### 2.1 标量
一个标量就是一个单独的数，一般用小写的的变量名称表示。
### 2.2 向量
一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排
列成一个方括号包围的纵柱：
![333](https://pic3.zhimg.com/80/v2-824f0739982920b1502ca01588d35e21_hd.jpg)
### 2.3 矩阵
矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如A。 如果一个实数矩阵高度为m，宽度为n，那么我们说$$A\epsilon R^{m\times n}$$ 。
![333](https://pic1.zhimg.com/80/v2-dd3017aa861f53973da40d860ec93732_hd.jpg)

矩阵这东西在机器学习中就不要太重要了！实际上，如果我们现在有N个用户的数据，每条数据含有M个特征，那其实它对应的就是一个N*M的矩阵呀；再比如，一张图由16*16的像素点组成，那这就是一个16*16的矩阵了。现在才发现，我们大一学的矩阵原理原来这么的有用！要是当时老师讲课的时候先普及一下，也不至于很多同学学矩阵的时候觉得莫名其妙了
### 2.4 张量
几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。

例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：
![22](https://pic1.zhimg.com/80/v2-c0c16793d4662bfcdd7e112030096f94_hd.jpg)


其中表的横轴表示图片的宽度值，这里只截取0~319；表的纵轴表示图片的高度值，这里只截取0~4；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为[1.0,1.0,1.0]，代表的就是RGB三原色在图片的这个位置的取值情况（即R=1.0，G=1.0，B=1.0）。

当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。

张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的
### 2.5 范数
有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数(norm) 的函数衡量矩阵大小。Lp 范数如下：

$$\left| \left| x \right| \right| _{p}^{} =\left( \sum_{i}^{}{\left| x_{i} \right| ^{p} } \right) _{}^{\frac{1}{p} } $$

所以：

L1范数$$\left| \left| x \right| \right|$$ ：为x向量各个元素绝对值之和；

L2范数$$\left| \left| x \right| \right| _{2}$$ ：为x向量各个元素平方和的开方。

这里先说明一下，在机器学习中，L1范数和L2范数很常见，主要用在损失函数中起到一个限制模型参数复杂度的作用，至于为什么要限制模型的复杂度，这又涉及到机器学习中常见的过拟合问题。具体的概念在后续文章中会有详细的说明和推导，大家先记住：这个东西很重要，实际中经常会涉及到，面试中也常会被问到！！！
### 2.6 特征分解
许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组**特征向量**和**特征值**。

方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量$$\nu$$ ：

$$A\nu =\lambda \nu $$

标量$$\lambda $$被称为这个特征向量对应的特征值。

使用特征分解去分析矩阵A时，得到特征向量构成的矩阵V和特征值构成的向量$$\lambda$$ ，我们可以重新将A写作：

$$A=Vdiag\left( \lambda \right) V^{-1} $$
### 2.7 奇异值分解（Singular Value Decomposition，SVD）
矩阵的特征分解是有前提条件的，那就是只有对可对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵，就是说连矩阵行和列的数目都不相等。这时候怎么办呢？人们将矩阵的特征分解进行推广，得到了一种叫作“矩阵的奇异值分解”的方法，简称SVD。通过奇异分解，我们会得到一些类似于特征分解的信息。

它的具体做法是将一个普通矩阵分解为奇异向量和奇异值。比如将矩阵A分解成三个矩阵的乘积：

$$A=UDV^{T} $$

假设A是一个$$m\times n$$矩阵，那么U是一个$$m\times m$$矩阵，D是一个$$m\times n$$矩阵，V是一个$$n\times n$$矩阵。

这些矩阵每一个都拥有特殊的结构，其中U和V都是正交矩阵，D是对角矩阵（注意，D不一定是方阵）。对角矩阵D对角线上的元素被称为矩阵A的奇异值。矩阵U的列向量被称为左奇异向量，矩阵V 的列向量被称右奇异向量。

SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，SVD可用于推荐系统中。
### 2.8 Moore-Penrose伪逆
对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程：

$$Ax=y$$

等式两边同时左乘左逆B后，得到：

$$x=By$$

是否存在唯一的映射将A映射到B取决于问题的形式。

如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。

Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为：
![1](https://pic1.zhimg.com/80/v2-1581c66947da5c30172f4ef80dd0b70f_hd.jpg)

但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：
![1](https://pic4.zhimg.com/80/v2-2845b623dc537e3bae0db22c4938e9c1_hd.jpg)

其中，矩阵U，D 和V 是矩阵A奇异值分解后得到的矩阵。对角矩阵D 的伪逆D+ 是其非零元素取倒之后再转置得到的。
### 2.9 几种常用的距离
上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。

设有两个n维变量$$A=\left[ x_{11}, x_{12},...,x_{1n} \right]$$ 和$$B=\left[ x_{21} ,x_{22} ,...,x_{2n} \right] $$，则一些常用的距离公式定义如下：
#### 2.9.1 曼哈顿距离

曼哈顿距离也称为城市街区距离，数学定义如下：

$$d_{12} =\sum_{k=1}^{n}{\left| x_{1k}-x_{2k} \right| } $$

曼哈顿距离的Python实现：

``` python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sum(abs(vector1-vector2)))
```
#### 2.9.2 欧氏距离

欧氏距离其实就是L2范数，数学定义如下：

$$d_{12} =\sqrt{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{2} } } $$

欧氏距离的Python实现：
```python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sqrt((vector1-vector2)*(vector1-vector2).T))
```
#### 2.9.3 闵可夫斯基距离

从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：

$$d_{12} =\sqrt[p]{\sum_{k=1}^{n}{\left( x_{1k} -x_{2k} \right) ^{p} } } $$

实际上，当p=1时，就是曼哈顿距离；当p=2时，就是欧式距离。



#### 2.9.4 切比雪夫距离

切比雪夫距离就是$$L_{\varpi}$$ ，即无穷范数，数学表达式如下：

$$d_{12} =max\left( \left| x_{1k}-x_{2k} \right| \right) $$

切比雪夫距离额Python实现如下：
```python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(sqrt(abs(vector1-vector2).max))
```

#### 2.9.5 夹角余弦

夹角余弦的取值范围为[-1,1]，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。

机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下：

$$cos\theta =\frac{AB}{\left| A \right| \left|B \right| } =\frac{\sum_{k=1}^{n}{x_{1k}x_{2k} } }{\sqrt{\sum_{k=1}^{n}{x_{1k}^{2} } } \sqrt{\sum_{k=1}^{n}{x_{2k}^{2} } } } $$

夹角余弦的Python实现：
```python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print(dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2)))
```
#### 2.9.6 汉明距离

汉明距离定义的是两个字符串中不相同位数的数目。

例如：字符串‘1111’与‘1001’之间的汉明距离为2。

信息编码中一般应使得编码间的汉明距离尽可能的小。

汉明距离的Python实现：
```python
from numpy import *
matV = mat([1,1,1,1],[1,0,0,1])
smstr = nonzero(matV[0]-matV[1])
print(smstr)
```
test
#### 2.9.7 杰卡德相似系数

两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号J(A,B)表示，数学表达式为：

$$J\left( A,B \right) =\frac{\left| A\cap B\right| }{\left|A\cup B \right| } $$

杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。

#### 2.9.8 杰卡德距离

与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：

$$J_{\sigma} =1-J\left( A,B \right) =\frac{\left| A\cup B \right| -\left| A\cap B \right| }{\left| A\cup B \right| } $$

杰卡德距离的Python实现：
```python
from numpy import *
import scipy.spatial.distance as dist
matV = mat([1,1,1,1],[1,0,0,1])
print(dist.pdist(matV,'jaccard'))
```
